{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CMPE256_RecSys_II_Problems.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KunjParikh/cmpe256/blob/master/HWs/CMPE256_RecSys_II_Problems.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdXMPM1J-tAM",
        "colab_type": "code",
        "outputId": "fc10809c-341d-42d4-91b2-5332a601f103",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "source": [
        "#Need this cell only for google colab envt.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!ln -s gdrive/'My Drive'/SJSU/'large scale analytics'/HWs gdata"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t18tw1Xp_JtD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Modules used.\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Atdf-G8rGBgH",
        "colab_type": "text"
      },
      "source": [
        "## (30%) Finding similiarity\n",
        "Distance based metrics are commonly used methods to create similiarity between two items. This similarity will be applied in content-based recommendation. They also have been used for a long time in Information Retrieval methods.\n",
        "\n",
        "Here is a good article on [Distance Metrics for Fun and Profit](http://www.benfrederickson.com/distance-metrics/) by Ben on better understanding of these metrics\n",
        "\n",
        "Could you based on the provided dataset (stories.csv) to generate similarity matrix between each story? The i, j, cell of this matrix will store the similarity value between story i and story j. You can define your own similarity measures.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mehmRgJOGBgP",
        "colab_type": "text"
      },
      "source": [
        "#### Solutions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poa7_Gs-GBgS",
        "colab_type": "code",
        "outputId": "c137dc40-8926-4ab3-9729-84c0f27f9065",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "stories = pd.read_csv('gdata/stories.csv', header=0, sep=',', quotechar='\"')\n",
        "stories = stories.drop(['score', 'user'], axis=1)\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "vector = vectorizer.fit_transform(stories['title'])\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "similarity = pd.DataFrame(cosine_similarity(vector))\n",
        "similarity.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>929</th>\n",
              "      <th>930</th>\n",
              "      <th>931</th>\n",
              "      <th>932</th>\n",
              "      <th>933</th>\n",
              "      <th>934</th>\n",
              "      <th>935</th>\n",
              "      <th>936</th>\n",
              "      <th>937</th>\n",
              "      <th>938</th>\n",
              "      <th>939</th>\n",
              "      <th>940</th>\n",
              "      <th>941</th>\n",
              "      <th>942</th>\n",
              "      <th>943</th>\n",
              "      <th>944</th>\n",
              "      <th>945</th>\n",
              "      <th>946</th>\n",
              "      <th>947</th>\n",
              "      <th>948</th>\n",
              "      <th>949</th>\n",
              "      <th>950</th>\n",
              "      <th>951</th>\n",
              "      <th>952</th>\n",
              "      <th>953</th>\n",
              "      <th>954</th>\n",
              "      <th>955</th>\n",
              "      <th>956</th>\n",
              "      <th>957</th>\n",
              "      <th>958</th>\n",
              "      <th>959</th>\n",
              "      <th>960</th>\n",
              "      <th>961</th>\n",
              "      <th>962</th>\n",
              "      <th>963</th>\n",
              "      <th>964</th>\n",
              "      <th>965</th>\n",
              "      <th>966</th>\n",
              "      <th>967</th>\n",
              "      <th>968</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.048141</td>\n",
              "      <td>0.099506</td>\n",
              "      <td>0.105550</td>\n",
              "      <td>0.03993</td>\n",
              "      <td>0.070175</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.078036</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.064623</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.029574</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.039899</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.098278</td>\n",
              "      <td>0.071919</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.067617</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.061053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.031600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.030631</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.032481</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.074386</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.116397</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.038086</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.045451</td>\n",
              "      <td>0.034058</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.132666</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.084279</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.067534</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.037606</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.102152</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.031952</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.048141</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0316</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.050379</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.031009</td>\n",
              "      <td>0.026019</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.060252</td>\n",
              "      <td>0.030509</td>\n",
              "      <td>0.027389</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.036952</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.033754</td>\n",
              "      <td>0.067055</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.036409</td>\n",
              "      <td>0.027282</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.033034</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.032072</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.028517</td>\n",
              "      <td>0.054097</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.035175</td>\n",
              "      <td>0.030124</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.026886</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.040723</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.025595</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.028632</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.056924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.099506</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.159184</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.039976</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.045526</td>\n",
              "      <td>0.065378</td>\n",
              "      <td>0.137536</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.032351</td>\n",
              "      <td>0.038985</td>\n",
              "      <td>0.053912</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.060705</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.028679</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.040184</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.067746</td>\n",
              "      <td>0.028438</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.067002</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.041354</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.039717</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.029835</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.037229</td>\n",
              "      <td>0.036433</td>\n",
              "      <td>0.030653</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.031675</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.026362</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.035137</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.032444</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.041446</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.041596</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.132402</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.030939</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.039065</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 969 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0    1       2         3    ...       965       966  967       968\n",
              "0  1.000000  0.0  0.0000  0.048141  ...  0.000000  0.000000  0.0  0.061053\n",
              "1  0.000000  1.0  0.0000  0.000000  ...  0.000000  0.000000  0.0  0.000000\n",
              "2  0.000000  0.0  1.0000  0.031600  ...  0.000000  0.000000  0.0  0.000000\n",
              "3  0.048141  0.0  0.0316  1.000000  ...  0.028632  0.000000  0.0  0.056924\n",
              "4  0.099506  0.0  0.0000  0.000000  ...  0.000000  0.039065  0.0  0.000000\n",
              "\n",
              "[5 rows x 969 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "fAr4lMulGBgh",
        "colab_type": "text"
      },
      "source": [
        "## (40%) Model - Matrix Completion by Alternating Least Square (ALS)\n",
        "\n",
        "Since in practical, customers may refuse to rate some items, study following materials. \n",
        "\n",
        "For a better and visual understanding of Matrix Factorization Techniques, read the following links\n",
        "- The original paper on [Collaborative Filtering for Implicit Feedback Datasets](http://yifanhu.net/PUB/cf.pdf)\n",
        "- [Finding Similar Music using Matrix Factorization](http://www.benfrederickson.com/matrix-factorization/) by Ben Frederickson\n",
        "- [Intro to Implicit Matrix Factorization: Classic ALS with Sketchfab Models](http://blog.ethanrosenthal.com/2016/10/19/implicit-mf-part-1/)\n",
        "\n",
        "We will apply ALS method to fill those missing values (put as 0) for following matrix, a rating matrix for six customers (rows number, say 1,2,3,4,5,6) to seven items (columns number, say A,B,C,D,E,F,G):\n",
        "\n",
        "R = np.array([[5, 0, 5, 0, 0, 1, 2], [0, 4, 0, 0, 2, 2, 1], [1, 0, 0, 1, 0, 1, 0], [0, 0, 1, 0, 1, 1, 0], [0, 2, 1, 0, 0, 1, 0], [0, 0, 3, 0, 1, 2, 0]])\n",
        "\n",
        "**Decomposition of the rating matrix**  \n",
        "R -> m users, n items  \n",
        "Row in P -> user's affinity to the features (m users, f latent factors/features)  \n",
        "Row in Q -> item's relation to the features (n items, f latent factors/features)  \n",
        "\n",
        "- P indicates how much user likes f latent factors  \n",
        "- Q indicates how much one item obtains f latent factors\n",
        "- The dot product indicates how much user likes item  \n",
        "- The decomposition automatically ranks features by their impact on the ratings\n",
        "- Features may not be intuitive though !\n",
        "- Model has hyperparameters (regularization, learning rate)\n",
        "\n",
        "Your task is to write a function using iterative method, e.g., stochastic gradient descent, to factorize\n",
        "matrix as P with dimension (6 x K) and Q with dimension (K x 7), where K is the number of latent features. \n",
        "After you get two factorized matrix P and Q, try to multiply them back, could you recommend the top three items of second customers based on this new rating matrix rating scores (0 value cells will have rating scores now). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VShMhBZkGBgk",
        "colab_type": "text"
      },
      "source": [
        "#### Solutions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXczXEg4GBgn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aclDKmzMGBgz",
        "colab_type": "text"
      },
      "source": [
        "## (20%) Learning to Rank\n",
        "\n",
        "**Bayesian Personalized Ranking pairwise loss:**\n",
        "\n",
        "Maximises the prediction difference between a positive example and a randomly chosen negative example. Useful when only positive interactions are present and optimising ROC AUC is desired.\n",
        "\n",
        "More details can be found for BPR at \n",
        "\n",
        "https://www.coursera.org/lecture/matrix-factorization/personalized-ranking-with-daniel-kluver-s3XJo\n",
        "\n",
        "https://www.slideshare.net/zenogantner/bayesian-personalized-ranking-for-nonuniformly-sampled-items\n",
        "\n",
        "**Weighted Approximate-Rank Pairwise loss**:  \n",
        "Maximises the rank of positive examples by repeatedly sampling negative examples until rank violating one is found. Useful when only positive interactions are present and optimising the top of the recommendation list (precision@k) is desired.WARP deals with (user, positive item, negative item) triplets. \n",
        "\n",
        "See following for WARP\n",
        "\n",
        "https://medium.com/@gabrieltseng/intro-to-warp-loss-automatic-differentiation-and-pytorch-b6aa5083187a\n",
        "\n",
        "\n",
        "This procedure yields roughly the following algorithm:\n",
        "\n",
        "- For a given (user, positive item pair), sample a negative item at random from all the remaining items. Compute predictions for both items; if the negative item’s prediction exceeds that of the positive item plus a margin, perform a gradient update to rank the positive item higher and the negative item lower. If there is no rank violation, continue sampling negative items until a violation is found.\n",
        "\n",
        "- If you found a violating negative example at the first try, make a large gradient update: this indicates that a lot of negative items are ranked higher than positives items given the current state of the model, and the model must be updated by a large amount. If it took a lot of sampling to find a violating example, perform a small update: the model is likely close to the optimum and should be updated at a low rate.\n",
        "\n",
        "You can install lightfm package and use its fetch_movielens data sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6__DLu8HGBg1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Get the data\n",
        "\n",
        "import numpy as np\n",
        "from lightfm.datasets import fetch_movielens\n",
        "from lightfm import LightFM\n",
        "from lightfm.evaluation import precision_at_k\n",
        "from lightfm.evaluation import auc_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtfT39GKGBg7",
        "colab_type": "text"
      },
      "source": [
        "**Movie lens dataset:**\n",
        "\n",
        "![](img/movielens.png)\n",
        "GroupLens Research has collected and made available rating data sets from the MovieLens web site (http://movielens.org). The data sets were collected over various periods of time, depending on the size of the set.\n",
        "\n",
        "http://grouplens.org/datasets/movielens/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrY4h7UMGBg_",
        "colab_type": "text"
      },
      "source": [
        "**Fetch movielens 100k dataset**\n",
        "\n",
        "The dataset contains 100,000 interactions from 1000 users on 1700 movies, and is exhaustively described\n",
        "#in its README http://files.grouplens.org/datasets/movielens/ml-100k-README.txt\n",
        "\n",
        "This data set consists of:\n",
        "- 100,000 ratings (1-5) from 943 users on 1682 movies. \n",
        "- Each user has rated at least 20 movies. \n",
        "- Simple demographic info for the users (age, gender, occupation, zip)\n",
        "\n",
        "### Questions 1: \n",
        "\n",
        "Using lightfm package builtin evaluation methods, e.g., precision_at_k and auc_score\n",
        "to evaluate recommendation performance from Bayesian Personalized Ranking pairwise loss, and Weighted Approximate-Rank Pairwise loss.  \n",
        "\n",
        "### Questions 2: \n",
        "\n",
        "Can you build a recommendation system based on Weighted Approximate-Rank Pairwise loss and perform recommendation for used with id [3, 25, 450]? You should show both known preference of used and predictions results (recommendatdions).\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnqzWyNJGBhC",
        "colab_type": "text"
      },
      "source": [
        "#### Solutions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOGRwTmFGBhG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lNOVoJaGBhM",
        "colab_type": "text"
      },
      "source": [
        "## (10%) Can you implement Discounted cumulative gain (DCG) and Normalized DCG. Then use example of ranking list to demonstrate your implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84SYxZr6GBhU",
        "colab_type": "text"
      },
      "source": [
        "#### Solutions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GrJ-c8YGBhX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}